# üéì Exerc√≠cios de Big Data - Spark + Iceberg + HDFS

Este diret√≥rio cont√©m 20 exerc√≠cios pr√°ticos de Big Data usando Apache Spark, Apache Iceberg e HDFS.

## üìã Lista de Exerc√≠cios

### B√°sicos (DataFrame e HDFS)
- **Exerc√≠cio 01**: Criar DataFrame simples com dados de funcion√°rios
- **Exerc√≠cio 02**: Salvar DataFrame no HDFS como CSV
- **Exerc√≠cio 03**: Ler CSV do HDFS
- **Exerc√≠cio 04**: Criar namespace Iceberg "exercicios"
- **Exerc√≠cio 05**: Criar tabela Iceberg "funcionarios"
- **Exerc√≠cio 06**: Inserir dados na tabela Iceberg

### Intermedi√°rios (Opera√ß√µes de Tabela)
- **Exerc√≠cio 07**: Consultar dados da tabela Iceberg
- **Exerc√≠cio 08**: Atualizar dados espec√≠ficos
- **Exerc√≠cio 09**: Deletar registros
- **Exerc√≠cio 10**: Fazer merge (upsert) de dados
- **Exerc√≠cio 11**: Criar tabela particionada por departamento
- **Exerc√≠cio 12**: Inserir dados na tabela particionada
- **Exerc√≠cio 13**: Consultar tabela particionada
- **Exerc√≠cio 14**: Demonstrar time travel

### Avan√ßados (Performance e Analytics)
- **Exerc√≠cio 15**: Criar tabela Iceberg a partir de DataFrame
- **Exerc√≠cio 16**: Fazer join entre tabelas Iceberg
- **Exerc√≠cio 17**: Criar view tempor√°ria e SQL complexo
- **Exerc√≠cio 18**: Exportar tabela Iceberg para CSV
- **Exerc√≠cio 19**: Otimizar tabela (compacta√ß√£o)
- **Exerc√≠cio 20**: Analytics com agrega√ß√µes complexas

## üöÄ Como Executar

### Pr√©-requisitos

1. **Ambiente Spark + Iceberg**: 
   - Apache Spark 3.5.x
   - Apache Iceberg 1.4.x
   - Hadoop HDFS
   - Hive Metastore

2. **Depend√™ncias Python**:
   ```bash
   pip install -r requirements.txt
   ```

3. **JARs do Iceberg**:
   - Download dos JARs necess√°rios:
     - `iceberg-spark-runtime-3.5_2.12-1.4.2.jar`
     - `iceberg-spark-extensions-3.5_2.12-1.4.2.jar`

### Configura√ß√£o do Ambiente

#### Op√ß√£o 1: Docker Compose (Recomendado)
```yaml
# docker-compose.yml
version: '3.8'
services:
  namenode:
    image: apache/hadoop:3.3.6
    ports:
      - "9000:9000"
      - "9870:9870"
    # ... configura√ß√£o HDFS
  
  hive-metastore:
    image: apache/hive:4.0.0
    ports:
      - "9083:9083"
    # ... configura√ß√£o Hive
```

#### Op√ß√£o 2: Configura√ß√£o Local
```bash
# Configurar vari√°veis de ambiente
export SPARK_HOME=/path/to/spark
export HADOOP_HOME=/path/to/hadoop
export JAVA_HOME=/path/to/java

# Adicionar JARs do Iceberg ao classpath
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/path/to/iceberg-jars/*
```

### Executando os Exerc√≠cios

#### Todos os exerc√≠cios principais:
```bash
python run_exercises.py
```

#### Exerc√≠cio espec√≠fico:
```bash
python run_exercises.py 1      # Executa exerc√≠cio 1
python run_exercises.py 15     # Executa exerc√≠cio 15
```

#### Todos os exerc√≠cios (1-20):
```bash
python run_exercises.py all
```

#### Ajuda:
```bash
python run_exercises.py help
```

### Executando exerc√≠cios individuais:
```bash
python exercicio_01.py
python exercicio_02.py
# ... etc
```

## üìÅ Estrutura dos Arquivos

```
exercicios/
‚îú‚îÄ‚îÄ config.py              # Configura√ß√µes compartilhadas
‚îú‚îÄ‚îÄ run_exercises.py        # Executor principal
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md              # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ exercicio_01.py        # Exerc√≠cio 1: DataFrame b√°sico
‚îú‚îÄ‚îÄ exercicio_02.py        # Exerc√≠cio 2: Salvar CSV
‚îú‚îÄ‚îÄ exercicio_03.py        # Exerc√≠cio 3: Ler CSV
‚îú‚îÄ‚îÄ exercicio_04.py        # Exerc√≠cio 4: Namespace Iceberg
‚îú‚îÄ‚îÄ exercicio_05.py        # Exerc√≠cio 5: Tabela Iceberg
‚îú‚îÄ‚îÄ exercicio_06.py        # Exerc√≠cio 6: Inserir dados
‚îú‚îÄ‚îÄ exercicio_11.py        # Exerc√≠cio 11: Tabela particionada
‚îú‚îÄ‚îÄ exercicio_15.py        # Exerc√≠cio 15: DataFrame para Iceberg
‚îî‚îÄ‚îÄ exercicio_18.py        # Exerc√≠cio 18: Exportar CSV
```

## üîß Configura√ß√£o

### Arquivo config.py
Cont√©m configura√ß√µes compartilhadas:
- URLs do HDFS (`hdfs://namenode:9000`)
- URLs do Hive Metastore (`thrift://hive-metastore:9083`)
- Configura√ß√£o do Spark com Iceberg
- Dados de exemplo para os exerc√≠cios

### Personaliza√ß√£o
Para adaptar a seu ambiente, edite `config.py`:
```python
def get_spark_iceberg_config():
    return {
        "spark.sql.catalog.iceberg": "org.apache.iceberg.spark.SparkCatalog",
        "spark.sql.catalog.iceberg.type": "hive",
        "spark.sql.catalog.iceberg.uri": "thrift://SEU-HIVE-METASTORE:9083",
        # ... outras configura√ß√µes
    }
```

## üêõ Troubleshooting

### Erro: "HDFS connection refused"
- Verifique se o HDFS est√° rodando na porta 9000
- Confirme a conectividade: `hdfs dfs -ls /`

### Erro: "Hive Metastore connection failed"
- Verifique se o Hive Metastore est√° rodando na porta 9083
- Confirme a conectividade com `telnet hive-metastore 9083`

### Erro: "Iceberg classes not found"
- Adicione os JARs do Iceberg ao classpath do Spark
- Use `--jars` ao executar spark-submit

### Erro: "PySpark not found"
- Instale PySpark: `pip install pyspark==3.5.0`
- Configure PYTHONPATH para incluir Spark

## üìö Recursos Adicionais

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
- [Hadoop HDFS Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)

## ü§ù Contribui√ß√£o

Para adicionar novos exerc√≠cios:
1. Crie arquivo `exercicio_XX.py` seguindo o padr√£o existente
2. Implemente fun√ß√£o `main()` que retorna True/False
3. Adicione tratamento de erros adequado
4. Atualize este README.md

## üìÑ Licen√ßa

Este projeto √© para fins educacionais. Use livremente para aprender Big Data com Spark e Iceberg!