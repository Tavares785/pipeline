# ğŸš€ ExercÃ­cios de Big Data - ImplementaÃ§Ã£o Completa

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o completa dos 20 exercÃ­cios de Big Data propostos, utilizando um ambiente Docker com Spark, Iceberg, HDFS, Hive Metastore, Trino e Superset.

## ğŸ“‹ Lista de ExercÃ­cios Implementados

### ExercÃ­cios BÃ¡sicos (1-3)
- âœ… **ExercÃ­cio 1**: Criar DataFrame simples com 3 linhas e 2 colunas
- âœ… **ExercÃ­cio 2**: Salvar DataFrame no HDFS como CSV
- âœ… **ExercÃ­cio 3**: Ler CSV do HDFS usando Spark

### ExercÃ­cios Iceberg - CRUD (4-10)
- âœ… **ExercÃ­cio 4**: Criar namespace Iceberg `lab.db`
- âœ… **ExercÃ­cio 5**: Criar tabela Iceberg `pessoas (id, nome)`
- âœ… **ExercÃ­cio 6**: Inserir 3 registros na tabela
- âœ… **ExercÃ­cio 7**: Consultar tabela Iceberg com SELECT
- âœ… **ExercÃ­cio 8**: Contar registros na tabela
- âœ… **ExercÃ­cio 9**: Atualizar registro usando UPDATE
- âœ… **ExercÃ­cio 10**: Deletar registro usando DELETE

### ExercÃ­cios AvanÃ§ados - Particionamento (11-13)
- âœ… **ExercÃ­cio 11**: Criar tabela particionada por ano
- âœ… **ExercÃ­cio 12**: Inserir dados com diferentes anos
- âœ… **ExercÃ­cio 13**: Consultar apenas uma partiÃ§Ã£o especÃ­fica

### ExercÃ­cios de Metadados e Versionamento (14-17)
- âœ… **ExercÃ­cio 14**: Ver metadados com DESCRIBE HISTORY/DETAIL
- âœ… **ExercÃ­cio 15**: Criar tabela Iceberg a partir de DataFrame
- âœ… **ExercÃ­cio 16**: Converter tabela Parquet para Iceberg
- âœ… **ExercÃ­cio 17**: Time Travel - acessar versÃµes anteriores

### ExercÃ­cios de IntegraÃ§Ã£o (18-20)
- âœ… **ExercÃ­cio 18**: Exportar tabela Iceberg para CSV no HDFS
- âœ… **ExercÃ­cio 19**: InstruÃ§Ãµes para Dashboard no Superset
- âœ… **ExercÃ­cio 20**: Queries via Trino para tabelas Iceberg

## ğŸ—ï¸ Arquitetura do Ambiente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jupyter Lab   â”‚    â”‚  Apache Spark   â”‚    â”‚      HDFS       â”‚
â”‚   (Port 8888)   â”‚â—„â”€â”€â–ºâ”‚  (Port 8080)    â”‚â—„â”€â”€â–ºâ”‚  (Port 9870)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Trino       â”‚    â”‚  Hive Metastore â”‚    â”‚   PostgreSQL    â”‚
â”‚   (Port 8082)   â”‚â—„â”€â”€â–ºâ”‚                 â”‚â—„â”€â”€â–ºâ”‚   (Port 5432)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Superset      â”‚
â”‚   (Port 8088)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Como Executar

### 1. Subir o Ambiente
```bash
# Navegar para o diretÃ³rio
cd /workspace

# Subir todos os containers
docker-compose up -d

# Verificar status
docker-compose ps
```

### 2. Acessar o Jupyter
```bash
# Abrir no navegador
http://localhost:8888

# Abrir o notebook
notebooks/exercicios_bigdata_completos.ipynb
```

### 3. Executar os ExercÃ­cios
- Execute as cÃ©lulas sequencialmente
- Cada exercÃ­cio estÃ¡ documentado e comentado
- Verifique os resultados em cada etapa

## ğŸ“Š Dados Criados

### Tabelas Iceberg
1. **lab.db.pessoas** - 3 registros de pessoas
2. **lab.db.vendas** - 9 registros particionados por ano (2022-2024)
3. **lab.db.produtos** - 5 produtos com categorias
4. **lab.db.clientes_iceberg** - 4 clientes convertidos do Parquet

### Arquivos HDFS
- `/data/ex1.csv` - Dados simples exportados
- `/export/vendas.csv` - Tabela vendas exportada
- `/export/produtos.csv` - Tabela produtos exportada

## ğŸ”§ Tecnologias Utilizadas

- **Apache Spark 3.5.1** - Engine de processamento
- **Apache Iceberg** - Formato de tabela com versionamento
- **HDFS** - Sistema de arquivos distribuÃ­do
- **Hive Metastore** - CatÃ¡logo de metadados
- **Trino** - Engine de query SQL distribuÃ­da
- **Apache Superset** - Plataforma de visualizaÃ§Ã£o
- **PostgreSQL** - Banco do Metastore
- **Docker & Docker Compose** - ContainerizaÃ§Ã£o

## ğŸŒ Interfaces Web

| ServiÃ§o | URL | DescriÃ§Ã£o |
|---------|-----|-----------|
| Jupyter Lab | http://localhost:8888 | Notebooks Python/PySpark |
| Spark Master | http://localhost:8080 | Interface do Spark |
| HDFS NameNode | http://localhost:9870 | Monitoramento HDFS |
| Trino | http://localhost:8082 | Interface de queries |
| Superset | http://localhost:8088 | Dashboards (admin/SenhaForte!123) |

## ğŸ“ˆ Exemplos de AnÃ¡lises

### Vendas por Ano
```sql
SELECT ano, COUNT(*) as vendas, SUM(valor) as receita 
FROM lab.db.vendas 
GROUP BY ano 
ORDER BY ano;
```

### Produtos por Categoria
```sql
SELECT categoria, COUNT(*) as total, AVG(preco) as preco_medio 
FROM lab.db.produtos 
GROUP BY categoria;
```

### Time Travel
```sql
SELECT * FROM lab.db.vendas VERSION AS OF 1;
```

## ğŸ› ï¸ Comandos Ãšteis

### Docker
```bash
# Ver logs
docker-compose logs -f

# Parar ambiente
docker-compose down

# Limpar volumes
docker-compose down -v
```

### HDFS
```bash
# Listar arquivos
docker exec -it namenode hdfs dfs -ls /

# Criar diretÃ³rio
docker exec -it namenode hdfs dfs -mkdir /data
```

### Spark
```bash
# Acessar container
docker exec -it spark-master bash

# Spark Shell
spark-shell --master spark://spark-master:7077
```

## ğŸ¯ Objetivos de Aprendizado

ApÃ³s completar estes exercÃ­cios, vocÃª terÃ¡ experiÃªncia prÃ¡tica com:

1. **Processamento DistribuÃ­do**: Spark DataFrames e SQL
2. **Armazenamento Moderno**: Formato Iceberg com ACID
3. **Data Lake**: HDFS para armazenamento escalÃ¡vel
4. **CatÃ¡logo de Dados**: Hive Metastore para metadados
5. **Query Engine**: Trino para consultas federadas
6. **VisualizaÃ§Ã£o**: Superset para dashboards
7. **Versionamento**: Time Travel e snapshots
8. **Particionamento**: OtimizaÃ§Ã£o de consultas
9. **ETL**: Extract, Transform, Load pipelines
10. **DevOps**: Docker e orquestraÃ§Ã£o de serviÃ§os

## ğŸ” PrÃ³ximos Passos

1. **Explore o Time Travel** - Experimente com diferentes versÃµes
2. **Crie Dashboards** - Use o Superset para visualizaÃ§Ãµes
3. **Otimize Queries** - Teste diferentes estratÃ©gias de particionamento
4. **Adicione Dados** - Importe seus prÃ³prios datasets
5. **Integre APIs** - Conecte fontes de dados externas

## ğŸ“š Recursos Adicionais

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
- [Trino Documentation](https://trino.io/docs/current/)
- [Apache Superset Documentation](https://superset.apache.org/docs/intro)

---

ğŸ‰ **ParabÃ©ns por completar todos os 20 exercÃ­cios!** 

Este ambiente fornece uma base sÃ³lida para projetos de Big Data em produÃ§Ã£o. Continue explorando e experimentando com diferentes cenÃ¡rios e casos de uso.