#!/usr/bin/env python3
"""
Exerc√≠cio 18: Exportar tabela Iceberg para CSV
Objetivo: Ler a tabela Iceberg e salvar para hdfs://namenode:9000/export/vendas.csv.
"""

from pyspark.sql import SparkSession

def main():
    # Criar sess√£o Spark com configura√ß√£o Iceberg
    spark = SparkSession.builder \
        .appName("Exercicio18_Exportar_Iceberg_CSV") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg.type", "hive") \
        .config("spark.sql.catalog.iceberg.uri", "thrift://hive-metastore:9083") \
        .config("spark.sql.catalog.iceberg.warehouse", "hdfs://namenode:9000/warehouse") \
        .getOrCreate()
    
    try:
        print("üì§ Exerc√≠cio 18: Exportando tabela Iceberg para CSV...")
        
        # Ler dados da tabela Iceberg
        df_vendas_export = spark.sql("SELECT * FROM iceberg.lab.db.vendas")
        
        print("üìä Dados a serem exportados:")
        df_vendas_export.show()
        
        # Caminho de destino no HDFS
        export_path = "hdfs://namenode:9000/export/vendas.csv"
        
        # Exportar para CSV
        df_vendas_export.coalesce(1) \
            .write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(export_path)
        
        print(f"‚úÖ Dados exportados com sucesso para: {export_path}")
        
        # Verificar se o arquivo foi criado
        try:
            import subprocess
            result = subprocess.run(['hdfs', 'dfs', '-ls', '/export/'], 
                                  capture_output=True, text=True)
            print("\nüìÅ Arquivos no diret√≥rio /export/:")
            print(result.stdout)
        except Exception as e:
            print(f"‚ö†Ô∏è N√£o foi poss√≠vel listar arquivos: {e}")
        
        # Verificar conte√∫do do arquivo exportado
        print("\nüîç Verificando dados exportados:")
        df_verificacao = spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(export_path)
        
        df_verificacao.show()
        print(f"üìà Total de registros exportados: {df_verificacao.count()}")
        
        print("üéâ Exerc√≠cio 18 conclu√≠do com sucesso!")
        
    except Exception as e:
        print(f"‚ùå Erro no exerc√≠cio 18: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()