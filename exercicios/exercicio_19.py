#!/usr/bin/env python3
"""
Exerc√≠cio 19: Otimizar tabela (compacta√ß√£o)
- Demonstrar compacta√ß√£o de arquivos no Iceberg
- Otimizar performance com OPTIMIZE
- Analisar m√©tricas antes e depois da otimiza√ß√£o
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

try:
    from config import create_spark_session
    from pyspark.sql import functions as F
except ImportError as e:
    print(f"‚ùå Erro ao importar depend√™ncias: {e}")
    print("üí° Certifique-se de que o PySpark est√° instalado: pip install pyspark")
    sys.exit(1)

def main():
    """
    Fun√ß√£o principal do exerc√≠cio 19
    """
    spark = None
    try:
        print("‚ö° Exerc√≠cio 19: Otimizar tabela (compacta√ß√£o)")
        print("-" * 50)
        
        # Criar sess√£o Spark
        spark = create_spark_session("Exercicio_19_Optimize_Table")
        
        # Verificar se a tabela existe
        try:
            count = spark.sql("SELECT COUNT(*) FROM iceberg.exercicios.funcionarios").collect()[0][0]
            print(f"‚úÖ Tabela funcionarios encontrada com {count} registros")
        except Exception as e:
            print(f"‚ö†Ô∏è Tabela funcionarios n√£o encontrada: {e}")
            print("üí° Execute primeiro os exerc√≠cios 4, 5 e 6 para criar a tabela")
            return False
        
        print("\n1Ô∏è‚É£ Verificando m√©tricas da tabela antes da otimiza√ß√£o:")
        
        try:
            # Verificar arquivos da tabela
            files_info = spark.sql("SELECT * FROM iceberg.exercicios.funcionarios.files")
            file_count = files_info.count()
            
            print(f"üìÅ Total de arquivos de dados: {file_count}")
            
            if file_count > 0:
                # Estat√≠sticas dos arquivos
                file_stats = files_info.agg(
                    F.sum("file_size_in_bytes").alias("total_size"),
                    F.avg("file_size_in_bytes").alias("avg_size"),
                    F.min("file_size_in_bytes").alias("min_size"),
                    F.max("file_size_in_bytes").alias("max_size"),
                    F.count("*").alias("file_count")
                ).collect()[0]
                
                print(f"üìä Tamanho total: {file_stats['total_size']:,} bytes")
                print(f"üìä Tamanho m√©dio: {file_stats['avg_size']:,.0f} bytes")
                print(f"üìä Menor arquivo: {file_stats['min_size']:,} bytes")
                print(f"üìä Maior arquivo: {file_stats['max_size']:,} bytes")
            
        except Exception as e:
            print(f"‚ö†Ô∏è N√£o foi poss√≠vel acessar informa√ß√µes de arquivos: {e}")
        
        print("\n2Ô∏è‚É£ Verificando snapshots antes da otimiza√ß√£o:")
        try:
            snapshots_before = spark.sql("SELECT * FROM iceberg.exercicios.funcionarios.snapshots")
            snapshot_count_before = snapshots_before.count()
            print(f"üì∑ Snapshots antes da otimiza√ß√£o: {snapshot_count_before}")
            
            print("üìã √öltimos 3 snapshots:")
            snapshots_before.select(
                "snapshot_id",
                F.from_unixtime(F.col("timestamp_ms")/1000).alias("timestamp"),
                "operation"
            ).orderBy(F.desc("timestamp_ms")).limit(3).show()
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao acessar snapshots: {e}")
        
        print("\n3Ô∏è‚É£ Adicionando mais dados para simular fragmenta√ß√£o:")
        
        # Adicionar alguns registros para criar mais arquivos
        for i in range(3):
            spark.sql(f"""
                INSERT INTO iceberg.exercicios.funcionarios 
                VALUES ({1000 + i}, 'Funcion√°rio Temp {i+1}', 'Cargo Temp', 'Temp', {3000.0 + i*100})
            """)
        
        print("‚úÖ Dados tempor√°rios adicionados")
        
        print("\n4Ô∏è‚É£ Verificando estado ap√≥s inser√ß√µes:")
        try:
            files_after_insert = spark.sql("SELECT * FROM iceberg.exercicios.funcionarios.files")
            file_count_after = files_after_insert.count()
            print(f"üìÅ Arquivos ap√≥s inser√ß√µes: {file_count_after}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar arquivos: {e}")
        
        print("\n5Ô∏è‚É£ Executando OPTIMIZE para compacta√ß√£o:")
        
        try:
            # Execute OPTIMIZE command
            optimize_result = spark.sql("CALL iceberg.system.rewrite_data_files('exercicios.funcionarios')")
            print("‚úÖ Comando OPTIMIZE executado")
            
            # Mostrar resultado da otimiza√ß√£o se dispon√≠vel
            try:
                optimize_result.show()
            except:
                print("‚ÑπÔ∏è Resultado da otimiza√ß√£o n√£o dispon√≠vel para exibi√ß√£o")
                
        except Exception as e:
            print(f"‚ö†Ô∏è OPTIMIZE n√£o dispon√≠vel, tentando m√©todo alternativo: {e}")
            
            # M√©todo alternativo: reescrever dados
            try:
                df = spark.table("iceberg.exercicios.funcionarios")
                df.coalesce(1).writeTo("iceberg.exercicios.funcionarios_optimized").createOrReplace()
                print("‚úÖ Tabela otimizada criada como alternativa")
            except Exception as e2:
                print(f"‚ö†Ô∏è M√©todo alternativo tamb√©m falhou: {e2}")
        
        print("\n6Ô∏è‚É£ Verificando m√©tricas ap√≥s otimiza√ß√£o:")
        
        try:
            files_after_optimize = spark.sql("SELECT * FROM iceberg.exercicios.funcionarios.files")
            file_count_optimized = files_after_optimize.count()
            
            print(f"üìÅ Arquivos ap√≥s otimiza√ß√£o: {file_count_optimized}")
            
            if file_count_optimized > 0:
                # Novas estat√≠sticas
                optimized_stats = files_after_optimize.agg(
                    F.sum("file_size_in_bytes").alias("total_size"),
                    F.avg("file_size_in_bytes").alias("avg_size"),
                    F.count("*").alias("file_count")
                ).collect()[0]
                
                print(f"üìä Novo tamanho total: {optimized_stats['total_size']:,} bytes")
                print(f"üìä Novo tamanho m√©dio: {optimized_stats['avg_size']:,.0f} bytes")
                print(f"üìä Redu√ß√£o de arquivos: {file_count - file_count_optimized}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar arquivos otimizados: {e}")
        
        print("\n7Ô∏è‚É£ Verificando novos snapshots:")
        try:
            snapshots_after = spark.sql("SELECT * FROM iceberg.exercicios.funcionarios.snapshots")
            snapshot_count_after = snapshots_after.count()
            print(f"üì∑ Snapshots ap√≥s otimiza√ß√£o: {snapshot_count_after}")
            
            print("üìã Snapshots mais recentes:")
            snapshots_after.select(
                "snapshot_id",
                F.from_unixtime(F.col("timestamp_ms")/1000).alias("timestamp"),
                "operation",
                "summary"
            ).orderBy(F.desc("timestamp_ms")).limit(5).show(truncate=False)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar snapshots: {e}")
        
        print("\n8Ô∏è‚É£ Testando performance de consulta:")
        
        import time
        
        # Consulta antes da limpeza
        start_time = time.time()
        result = spark.sql("""
            SELECT departamento, COUNT(*) as funcionarios, AVG(salario) as salario_medio
            FROM iceberg.exercicios.funcionarios
            GROUP BY departamento
            ORDER BY funcionarios DESC
        """)
        result.collect()  # For√ßa execu√ß√£o
        end_time = time.time()
        
        print(f"‚è±Ô∏è Tempo da consulta: {end_time - start_time:.3f} segundos")
        result.show()
        
        print("\n9Ô∏è‚É£ Limpando dados tempor√°rios:")
        
        # Remover dados tempor√°rios
        spark.sql("DELETE FROM iceberg.exercicios.funcionarios WHERE departamento = 'Temp'")
        print("‚úÖ Dados tempor√°rios removidos")
        
        print("\nüîü Verificando estado final:")
        final_count = spark.sql("SELECT COUNT(*) FROM iceberg.exercicios.funcionarios").collect()[0][0]
        print(f"üìä Registros finais: {final_count}")
        
        # Summary final
        try:
            final_files = spark.sql("SELECT * FROM iceberg.exercicios.funcionarios.files")
            final_file_count = final_files.count()
            
            print(f"üìÅ Arquivos finais: {final_file_count}")
            
            if final_file_count > 0:
                final_size = final_files.agg(F.sum("file_size_in_bytes")).collect()[0][0]
                print(f"üìä Tamanho final total: {final_size:,} bytes")
            
        except Exception as e:
            print(f"‚ö†Ô∏è N√£o foi poss√≠vel obter estat√≠sticas finais: {e}")
        
        print("\nüìà Benef√≠cios da otimiza√ß√£o:")
        print("‚úÖ Redu√ß√£o no n√∫mero de arquivos pequenos")
        print("‚úÖ Melhoria na performance de consultas")
        print("‚úÖ Redu√ß√£o do overhead de metadados")
        print("‚úÖ Melhor utiliza√ß√£o do cache")
        
        print("‚úÖ Exerc√≠cio 19 conclu√≠do com sucesso!")
        print("‚ö° Demonstramos otimiza√ß√£o e compacta√ß√£o de tabelas Iceberg")
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no exerc√≠cio 19: {e}")
        return False
    finally:
        if spark:
            spark.stop()

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)