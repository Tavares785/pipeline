{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34de26dc",
   "metadata": {},
   "source": [
    "# Exerc√≠cios de Big Data com Spark + Iceberg + Hive Metastore + HDFS\n",
    "\n",
    "Este notebook cont√©m as solu√ß√µes para os 20 exerc√≠cios de Big Data usando:\n",
    "- **Apache Spark** para processamento distribu√≠do\n",
    "- **Apache Iceberg** para formato de tabela\n",
    "- **Hive Metastore** para metadados\n",
    "- **HDFS** para armazenamento distribu√≠do\n",
    "\n",
    "**Pr√©-requisitos:**\n",
    "- Ambiente `lab` configurado com todos os servi√ßos\n",
    "- Jupyter Notebook rodando dentro do container\n",
    "- Conectividade com HDFS, Hive Metastore e cat√°logo Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353decb",
   "metadata": {},
   "source": [
    "## 1. Setup e Configura√ß√£o\n",
    "\n",
    "Primeiro, vamos configurar o Spark com o cat√°logo Iceberg e verificar a conectividade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necess√°rias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Criar sess√£o Spark com configura√ß√£o Iceberg\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataExercicios\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"hdfs://namenode:9000/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar n√≠vel de log\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark Session criada com sucesso!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog', 'spark_catalog')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar conectividade com HDFS\n",
    "try:\n",
    "    spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jvm.java.net.URI(\"hdfs://namenode:9000\"),\n",
    "        spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    ).listStatus(spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/\"))\n",
    "    print(\"‚úÖ HDFS acess√≠vel!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao acessar HDFS: {e}\")\n",
    "\n",
    "# Verificar cat√°logos dispon√≠veis\n",
    "print(\"\\nüìã Cat√°logos dispon√≠veis:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da85ce",
   "metadata": {},
   "source": [
    "## 2. Exerc√≠cio 1: Criar um DataFrame simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626575e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 1: Criar um DataFrame com tr√™s linhas e duas colunas (id, nome)\n",
    "data = [(1, \"Jo√£o\"), (2, \"Maria\"), (3, \"Pedro\")]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nome\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_simples = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"üìä DataFrame criado:\")\n",
    "df_simples.show()\n",
    "\n",
    "print(\"üìã Schema do DataFrame:\")\n",
    "df_simples.printSchema()\n",
    "\n",
    "print(f\"üìà N√∫mero de registros: {df_simples.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed905a80",
   "metadata": {},
   "source": [
    "## 3. Exerc√≠cio 2: Salvar DataFrame no HDFS como CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf51850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 2: Salvar DataFrame no HDFS como CSV\n",
    "hdfs_path = \"hdfs://namenode:9000/data/ex1.csv\"\n",
    "\n",
    "# Criar um DataFrame para salvar\n",
    "data_ex2 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"Diana\")]\n",
    "df_ex2 = spark.createDataFrame(data_ex2, schema)\n",
    "\n",
    "print(\"üìä DataFrame a ser salvo:\")\n",
    "df_ex2.show()\n",
    "\n",
    "# Salvar como CSV no HDFS (mode='overwrite' para sobrescrever se existir)\n",
    "df_ex2.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(hdfs_path)\n",
    "\n",
    "print(f\"‚úÖ DataFrame salvo em: {hdfs_path}\")\n",
    "\n",
    "# Verificar se o arquivo foi criado\n",
    "try:\n",
    "    # Listar arquivos no diret√≥rio\n",
    "    import subprocess\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-ls', '/data/'], \n",
    "                          capture_output=True, text=True)\n",
    "    print(\"üìÅ Arquivos no diret√≥rio /data/:\")\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel listar arquivos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8671807",
   "metadata": {},
   "source": [
    "## 4. Exerc√≠cio 3: Ler CSV do HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ce690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 3: Ler CSV do HDFS\n",
    "df_lido = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(hdfs_path)\n",
    "\n",
    "print(\"üìñ DataFrame lido do HDFS:\")\n",
    "df_lido.show()\n",
    "\n",
    "print(\"üìã Schema inferido:\")\n",
    "df_lido.printSchema()\n",
    "\n",
    "print(f\"üìà N√∫mero de registros lidos: {df_lido.count()}\")\n",
    "\n",
    "# Verificar se os dados s√£o iguais\n",
    "print(\"\\nüîç Compara√ß√£o com DataFrame original:\")\n",
    "print(\"DataFrame original:\")\n",
    "df_ex2.show()\n",
    "print(\"DataFrame lido:\")\n",
    "df_lido.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c3949",
   "metadata": {},
   "source": [
    "## 5. Exerc√≠cio 4: Criar namespace Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 4: Criar namespace Iceberg\n",
    "print(\"üóÇÔ∏è Criando namespace 'lab.db'...\")\n",
    "\n",
    "# Criar namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.lab\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.lab.db\")\n",
    "\n",
    "print(\"‚úÖ Namespace 'lab.db' criado com sucesso!\")\n",
    "\n",
    "# Verificar namespaces criados\n",
    "print(\"\\nüìã Namespaces dispon√≠veis no cat√°logo iceberg:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao mostrar namespaces: {e}\")\n",
    "    # Alternativa\n",
    "    spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8a33d",
   "metadata": {},
   "source": [
    "## 6. Exerc√≠cio 5: Criar tabela Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db38a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 5: Criar tabela Iceberg pessoas\n",
    "print(\"üóÉÔ∏è Criando tabela Iceberg 'pessoas'...\")\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg.lab.db.pessoas (\n",
    "    id INT,\n",
    "    nome STRING\n",
    ") USING ICEBERG\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_table_sql)\n",
    "print(\"‚úÖ Tabela 'lab.db.pessoas' criada com sucesso!\")\n",
    "\n",
    "# Verificar se a tabela foi criada\n",
    "print(\"\\nüìã Tabelas no namespace lab.db:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW TABLES IN iceberg.lab.db\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro: {e}\")\n",
    "    # Alternativa\n",
    "    spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Mostrar estrutura da tabela\n",
    "print(\"\\nüìä Estrutura da tabela pessoas:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lab.db.pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28c3f0",
   "metadata": {},
   "source": [
    "## 7. Exerc√≠cio 6: Inserir dados na tabela Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 6: Inserir dados na tabela Iceberg\n",
    "print(\"üìù Inserindo dados na tabela pessoas...\")\n",
    "\n",
    "# Inserir 3 registros\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO iceberg.lab.db.pessoas VALUES \n",
    "    (1, 'Alice'),\n",
    "    (2, 'Bob'),\n",
    "    (3, 'Charlie')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(insert_sql)\n",
    "print(\"‚úÖ Dados inseridos com sucesso!\")\n",
    "\n",
    "# Verificar inser√ß√£o\n",
    "print(\"\\nüìä Dados na tabela pessoas:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addc0cc",
   "metadata": {},
   "source": [
    "## 8. Exerc√≠cio 7: Ler tabela Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ecb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 7: Ler tabela Iceberg\n",
    "print(\"üìñ Lendo dados da tabela pessoas...\")\n",
    "\n",
    "# Query SELECT simples\n",
    "df_pessoas = spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\")\n",
    "df_pessoas.show()\n",
    "\n",
    "# Tamb√©m podemos usar a API DataFrame\n",
    "df_pessoas_api = spark.table(\"iceberg.lab.db.pessoas\")\n",
    "print(\"\\nüìä Usando API DataFrame:\")\n",
    "df_pessoas_api.show()\n",
    "\n",
    "# Mostrar informa√ß√µes do DataFrame\n",
    "print(f\"üìà Total de registros: {df_pessoas.count()}\")\n",
    "print(\"üìã Schema:\")\n",
    "df_pessoas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5673d0f",
   "metadata": {},
   "source": [
    "## 9. Exerc√≠cio 8: Contar registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 8: Contar registros\n",
    "print(\"üî¢ Contando registros na tabela pessoas...\")\n",
    "\n",
    "# Usando SQL COUNT\n",
    "count_result = spark.sql(\"SELECT COUNT(*) as total FROM iceberg.lab.db.pessoas\")\n",
    "count_result.show()\n",
    "\n",
    "# Capturar o valor para uso\n",
    "total_registros = count_result.collect()[0]['total']\n",
    "print(f\"üìä Total de registros na tabela pessoas: {total_registros}\")\n",
    "\n",
    "# Alternativa usando DataFrame API\n",
    "total_api = spark.table(\"iceberg.lab.db.pessoas\").count()\n",
    "print(f\"üìä Total usando DataFrame API: {total_api}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a44326",
   "metadata": {},
   "source": [
    "## 10. Exerc√≠cio 9: Atualizar um registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbf61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 9: Atualizar um registro\n",
    "print(\"‚úèÔ∏è Atualizando registro na tabela pessoas...\")\n",
    "\n",
    "# Mostrar dados antes da atualiza√ß√£o\n",
    "print(\"üìä Dados ANTES da atualiza√ß√£o:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "\n",
    "# Atualizar Alice para Alice Silva\n",
    "update_sql = \"\"\"\n",
    "UPDATE iceberg.lab.db.pessoas \n",
    "SET nome = 'Alice Silva' \n",
    "WHERE nome = 'Alice'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(update_sql)\n",
    "print(\"‚úÖ Registro atualizado com sucesso!\")\n",
    "\n",
    "# Mostrar dados ap√≥s a atualiza√ß√£o\n",
    "print(\"üìä Dados AP√ìS a atualiza√ß√£o:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e9b8b",
   "metadata": {},
   "source": [
    "## 11. Exerc√≠cio 10: Deletar um registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ef5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 10: Deletar um registro\n",
    "print(\"üóëÔ∏è Deletando registro da tabela pessoas...\")\n",
    "\n",
    "# Mostrar dados antes da exclus√£o\n",
    "print(\"üìä Dados ANTES da exclus√£o:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "\n",
    "# Deletar Bob\n",
    "delete_sql = \"\"\"\n",
    "DELETE FROM iceberg.lab.db.pessoas \n",
    "WHERE nome = 'Bob'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(delete_sql)\n",
    "print(\"‚úÖ Registro deletado com sucesso!\")\n",
    "\n",
    "# Mostrar dados ap√≥s a exclus√£o\n",
    "print(\"üìä Dados AP√ìS a exclus√£o:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "\n",
    "print(f\"üìà Total de registros ap√≥s exclus√£o: {spark.sql('SELECT COUNT(*) FROM iceberg.lab.db.pessoas').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2fb92",
   "metadata": {},
   "source": [
    "## 12. Exerc√≠cio 11: Criar tabela particionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 11: Criar tabela particionada por ano\n",
    "print(\"üóÇÔ∏è Criando tabela particionada 'vendas'...\")\n",
    "\n",
    "create_vendas_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg.lab.db.vendas (\n",
    "    id INT,\n",
    "    valor DOUBLE,\n",
    "    ano INT\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (ano)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_vendas_sql)\n",
    "print(\"‚úÖ Tabela 'vendas' particionada criada com sucesso!\")\n",
    "\n",
    "# Verificar estrutura da tabela\n",
    "print(\"\\nüìä Estrutura da tabela vendas:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lab.db.vendas\").show()\n",
    "\n",
    "# Mostrar informa√ß√µes de particionamento\n",
    "print(\"\\nüóÇÔ∏è Informa√ß√µes de particionamento:\")\n",
    "spark.sql(\"SHOW PARTITIONS iceberg.lab.db.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4bca5",
   "metadata": {},
   "source": [
    "## 13. Exerc√≠cio 12: Inserir dados particionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 12: Inserir dados particionados\n",
    "print(\"üìù Inserindo dados particionados na tabela vendas...\")\n",
    "\n",
    "insert_vendas_sql = \"\"\"\n",
    "INSERT INTO iceberg.lab.db.vendas VALUES \n",
    "    (1, 1000.50, 2022),\n",
    "    (2, 1500.75, 2022),\n",
    "    (3, 2000.00, 2023),\n",
    "    (4, 2500.25, 2023),\n",
    "    (5, 3000.00, 2023),\n",
    "    (6, 1750.80, 2024),\n",
    "    (7, 2200.60, 2024)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(insert_vendas_sql)\n",
    "print(\"‚úÖ Dados particionados inseridos com sucesso!\")\n",
    "\n",
    "# Verificar dados inseridos\n",
    "print(\"\\nüìä Dados na tabela vendas:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas ORDER BY ano, id\").show()\n",
    "\n",
    "# Verificar parti√ß√µes criadas\n",
    "print(\"\\nüóÇÔ∏è Parti√ß√µes criadas:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW PARTITIONS iceberg.lab.db.vendas\").show()\n",
    "except:\n",
    "    print(\"Informa√ß√£o de parti√ß√µes n√£o dispon√≠vel via SHOW PARTITIONS\")\n",
    "\n",
    "print(f\"\\nüìà Total de registros: {spark.sql('SELECT COUNT(*) FROM iceberg.lab.db.vendas').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc90febf",
   "metadata": {},
   "source": [
    "## 14. Exerc√≠cio 13: Consultar apenas um particionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 13: Consultar apenas vendas de 2023\n",
    "print(\"üîç Consultando apenas vendas do ano 2023...\")\n",
    "\n",
    "vendas_2023 = spark.sql(\"SELECT * FROM iceberg.lab.db.vendas WHERE ano = 2023\")\n",
    "vendas_2023.show()\n",
    "\n",
    "print(f\"üìä Total de vendas em 2023: {vendas_2023.count()}\")\n",
    "print(f\"üí∞ Valor total vendido em 2023: {vendas_2023.agg({'valor': 'sum'}).collect()[0][0]}\")\n",
    "\n",
    "# Comparar com outros anos\n",
    "print(\"\\nüìà Resumo por ano:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT ano, \n",
    "           COUNT(*) as qtd_vendas, \n",
    "           SUM(valor) as total_valor,\n",
    "           AVG(valor) as valor_medio\n",
    "    FROM iceberg.lab.db.vendas \n",
    "    GROUP BY ano \n",
    "    ORDER BY ano\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e14ff7",
   "metadata": {},
   "source": [
    "## 15. Exerc√≠cio 14: Ver metadados da tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb80bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 14: Ver metadados da tabela\n",
    "print(\"üìã Visualizando metadados da tabela vendas...\")\n",
    "\n",
    "# DESCRIBE HISTORY - hist√≥rico da tabela\n",
    "print(\"üìú Hist√≥rico da tabela (DESCRIBE HISTORY):\")\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE HISTORY iceberg.lab.db.vendas\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao mostrar hist√≥rico: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# DESCRIBE DETAIL - detalhes da tabela  \n",
    "print(\"üîç Detalhes da tabela (DESCRIBE DETAIL):\")\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE DETAIL iceberg.lab.db.vendas\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao mostrar detalhes: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Informa√ß√µes adicionais\n",
    "print(\"‚ÑπÔ∏è Informa√ß√µes adicionais:\")\n",
    "print(\"üìä Estrutura da tabela:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lab.db.vendas\").show()\n",
    "\n",
    "print(\"üìÅ Propriedades da tabela:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW TBLPROPERTIES iceberg.lab.db.vendas\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Propriedades n√£o dispon√≠veis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee42ba1",
   "metadata": {},
   "source": [
    "## 16. Exerc√≠cio 15: Criar tabela Iceberg a partir de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399611ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 15: Criar tabela Iceberg a partir de DataFrame\n",
    "print(\"üîÑ Criando tabela Iceberg diretamente a partir de DataFrame...\")\n",
    "\n",
    "# Criar DataFrame artificial com dados de produtos\n",
    "data_produtos = [\n",
    "    (1, \"Notebook\", 2500.00, \"Eletr√¥nicos\"),\n",
    "    (2, \"Mouse\", 50.00, \"Eletr√¥nicos\"),\n",
    "    (3, \"Teclado\", 150.00, \"Eletr√¥nicos\"),\n",
    "    (4, \"Monitor\", 800.00, \"Eletr√¥nicos\"),\n",
    "    (5, \"Cadeira\", 400.00, \"M√≥veis\"),\n",
    "    (6, \"Mesa\", 600.00, \"M√≥veis\")\n",
    "]\n",
    "\n",
    "schema_produtos = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"preco\", DoubleType(), True),\n",
    "    StructField(\"categoria\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_produtos = spark.createDataFrame(data_produtos, schema_produtos)\n",
    "\n",
    "print(\"üìä DataFrame produtos criado:\")\n",
    "df_produtos.show()\n",
    "\n",
    "# Criar tabela Iceberg usando writeTo()\n",
    "df_produtos.writeTo(\"iceberg.lab.db.tabela_df\").createOrReplace()\n",
    "print(\"‚úÖ Tabela 'tabela_df' criada com sucesso a partir do DataFrame!\")\n",
    "\n",
    "# Verificar tabela criada\n",
    "print(\"\\nüìã Dados na nova tabela:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.tabela_df\").show()\n",
    "\n",
    "print(\"\\nüìä Estat√≠sticas por categoria:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT categoria, \n",
    "           COUNT(*) as qtd_produtos,\n",
    "           AVG(preco) as preco_medio,\n",
    "           MAX(preco) as preco_maximo\n",
    "    FROM iceberg.lab.db.tabela_df \n",
    "    GROUP BY categoria\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7800f5d",
   "metadata": {},
   "source": [
    "## 17. Exerc√≠cio 16: Converter tabela para Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 16: Converter tabela Parquet para Iceberg\n",
    "print(\"üîÑ Criando tabela Parquet e convertendo para Iceberg...\")\n",
    "\n",
    "# Primeiro, criar uma tabela Parquet simples\n",
    "data_temp = [\n",
    "    (1, \"Dados tempor√°rios 1\"),\n",
    "    (2, \"Dados tempor√°rios 2\"),\n",
    "    (3, \"Dados tempor√°rios 3\")\n",
    "]\n",
    "\n",
    "df_temp = spark.createDataFrame(data_temp, StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"descricao\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# Salvar como tabela Parquet\n",
    "df_temp.write.mode(\"overwrite\").saveAsTable(\"temp_parquet\")\n",
    "print(\"‚úÖ Tabela Parquet 'temp_parquet' criada!\")\n",
    "\n",
    "# Verificar tabela criada\n",
    "print(\"\\nüìä Dados na tabela Parquet:\")\n",
    "spark.sql(\"SELECT * FROM temp_parquet\").show()\n",
    "\n",
    "# Converter para Iceberg (simula√ß√£o - o comando exato pode variar)\n",
    "print(\"\\nüîÑ Convertendo para formato Iceberg...\")\n",
    "try:\n",
    "    # M√©todo 1: Recriar como Iceberg\n",
    "    df_temp.writeTo(\"iceberg.lab.db.temp_iceberg\").createOrReplace()\n",
    "    print(\"‚úÖ Tabela convertida para Iceberg como 'temp_iceberg'!\")\n",
    "    \n",
    "    # Verificar convers√£o\n",
    "    print(\"\\nüìä Dados na tabela Iceberg convertida:\")\n",
    "    spark.sql(\"SELECT * FROM iceberg.lab.db.temp_iceberg\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro na convers√£o: {e}\")\n",
    "    print(\"üí° Nota: A convers√£o direta pode requerer configura√ß√µes espec√≠ficas do ambiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fc008",
   "metadata": {},
   "source": [
    "## 18. Exerc√≠cio 17: Leitura incremental (Time Travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f01648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 17: Time Travel - Leitura incremental\n",
    "print(\"‚è∞ Demonstrando funcionalidade Time Travel do Iceberg...\")\n",
    "\n",
    "# Primeiro, vamos ver o hist√≥rico atual da tabela vendas\n",
    "print(\"üìú Hist√≥rico atual da tabela vendas:\")\n",
    "try:\n",
    "    history_df = spark.sql(\"DESCRIBE HISTORY iceberg.lab.db.vendas\")\n",
    "    history_df.show(truncate=False)\n",
    "    \n",
    "    # Capturar snapshots dispon√≠veis\n",
    "    snapshots = history_df.collect()\n",
    "    if len(snapshots) > 0:\n",
    "        print(f\"\\nüìä Total de snapshots dispon√≠veis: {len(snapshots)}\")\n",
    "        \n",
    "        # Tentar acessar vers√£o anterior (snapshot 1)\n",
    "        print(\"\\n‚èÆÔ∏è Acessando dados da vers√£o 1:\")\n",
    "        try:\n",
    "            spark.sql(\"SELECT * FROM iceberg.lab.db.vendas VERSION AS OF 1\").show()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è N√£o foi poss√≠vel acessar vers√£o 1: {e}\")\n",
    "            \n",
    "        # Mostrar dados da vers√£o atual\n",
    "        print(\"\\nüìä Dados da vers√£o atual:\")\n",
    "        spark.sql(\"SELECT * FROM iceberg.lab.db.vendas\").show()\n",
    "        \n",
    "        # Exemplo de time travel por timestamp (se dispon√≠vel)\n",
    "        print(\"\\n‚è∞ Tentando time travel por timestamp...\")\n",
    "        try:\n",
    "            # Pegar timestamp do primeiro snapshot\n",
    "            first_snapshot = snapshots[-1]  # Mais antigo\n",
    "            timestamp = first_snapshot['made_current_at']\n",
    "            print(f\"Tentando acessar dados do timestamp: {timestamp}\")\n",
    "            spark.sql(f\"SELECT * FROM iceberg.lab.db.vendas TIMESTAMP AS OF '{timestamp}'\").show()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Time travel por timestamp n√£o dispon√≠vel: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhum snapshot encontrado\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao acessar hist√≥rico: {e}\")\n",
    "    print(\"üí° Time Travel requer que existam m√∫ltiplas vers√µes da tabela\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab4430",
   "metadata": {},
   "source": [
    "## 19. Exerc√≠cio 18: Exportar tabela Iceberg para CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 18: Exportar tabela Iceberg para CSV\n",
    "print(\"üì§ Exportando tabela Iceberg para CSV no HDFS...\")\n",
    "\n",
    "# Ler dados da tabela Iceberg\n",
    "df_vendas_export = spark.sql(\"SELECT * FROM iceberg.lab.db.vendas\")\n",
    "\n",
    "print(\"üìä Dados a serem exportados:\")\n",
    "df_vendas_export.show()\n",
    "\n",
    "# Caminho de destino no HDFS\n",
    "export_path = \"hdfs://namenode:9000/export/vendas.csv\"\n",
    "\n",
    "# Exportar para CSV\n",
    "df_vendas_export.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(export_path)\n",
    "\n",
    "print(f\"‚úÖ Dados exportados com sucesso para: {export_path}\")\n",
    "\n",
    "# Verificar se o arquivo foi criado\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-ls', '/export/'], \n",
    "                          capture_output=True, text=True)\n",
    "    print(\"\\nüìÅ Arquivos no diret√≥rio /export/:\")\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel listar arquivos: {e}\")\n",
    "\n",
    "# Verificar conte√∫do do arquivo exportado\n",
    "print(\"\\nüîç Verificando dados exportados:\")\n",
    "df_verificacao = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(export_path)\n",
    "\n",
    "df_verificacao.show()\n",
    "print(f\"üìà Total de registros exportados: {df_verificacao.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c2150",
   "metadata": {},
   "source": [
    "## 20. Exerc√≠cio 19: Configura√ß√£o para Dashboard no Superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 19: Prepara√ß√£o para Dashboard no Superset\n",
    "print(\"üìä Preparando dados para Dashboard no Superset...\")\n",
    "\n",
    "# Criar uma view agregada para facilitar visualiza√ß√µes\n",
    "print(\"üìà Criando view agregada para dashboards...\")\n",
    "\n",
    "create_view_sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW iceberg.lab.db.vendas_dashboard AS\n",
    "SELECT \n",
    "    ano,\n",
    "    COUNT(*) as total_vendas,\n",
    "    SUM(valor) as receita_total,\n",
    "    AVG(valor) as ticket_medio,\n",
    "    MIN(valor) as menor_venda,\n",
    "    MAX(valor) as maior_venda\n",
    "FROM iceberg.lab.db.vendas\n",
    "GROUP BY ano\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_view_sql)\n",
    "print(\"‚úÖ View 'vendas_dashboard' criada!\")\n",
    "\n",
    "# Verificar dados da view\n",
    "print(\"\\nüìä Dados da view para dashboard:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas_dashboard ORDER BY ano\").show()\n",
    "\n",
    "# Criar dados de exemplo adicionais para visualiza√ß√µes mais ricas\n",
    "print(\"\\nüìù Criando tabela de vendas detalhadas para dashboards...\")\n",
    "\n",
    "vendas_detalhadas_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE iceberg.lab.db.vendas_detalhadas\n",
    "USING ICEBERG AS\n",
    "SELECT \n",
    "    id,\n",
    "    valor,\n",
    "    ano,\n",
    "    CASE \n",
    "        WHEN valor < 1000 THEN 'Baixo'\n",
    "        WHEN valor < 2000 THEN 'M√©dio' \n",
    "        ELSE 'Alto'\n",
    "    END as categoria_valor,\n",
    "    CASE \n",
    "        WHEN id % 4 = 0 THEN 'Q1'\n",
    "        WHEN id % 4 = 1 THEN 'Q2'\n",
    "        WHEN id % 4 = 2 THEN 'Q3'\n",
    "        ELSE 'Q4'\n",
    "    END as trimestre,\n",
    "    valor * 0.1 as comissao\n",
    "FROM iceberg.lab.db.vendas\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(vendas_detalhadas_sql)\n",
    "print(\"‚úÖ Tabela 'vendas_detalhadas' criada!\")\n",
    "\n",
    "print(\"\\nüìä Preview dos dados detalhados:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas_detalhadas ORDER BY ano, id\").show()\n",
    "\n",
    "print(\"\"\"\n",
    "üìã INSTRU√á√ïES PARA SUPERSET:\n",
    "\n",
    "1. Conectar ao Trino:\n",
    "   - Host: trino (ou IP do container Trino)\n",
    "   - Porta: 8080\n",
    "   - Usu√°rio: admin\n",
    "   - Cat√°logo: iceberg\n",
    "   - Schema: lab.db\n",
    "\n",
    "2. URL de conex√£o no Superset:\n",
    "   trino://admin@trino:8080/iceberg/lab.db\n",
    "\n",
    "3. Tabelas dispon√≠veis para visualiza√ß√£o:\n",
    "   - iceberg.lab.db.vendas (dados brutos)\n",
    "   - iceberg.lab.db.vendas_dashboard (dados agregados)\n",
    "   - iceberg.lab.db.vendas_detalhadas (dados com categorias)\n",
    "   - iceberg.lab.db.pessoas (dados de pessoas)\n",
    "   - iceberg.lab.db.tabela_df (dados de produtos)\n",
    "\n",
    "4. Exemplos de visualiza√ß√µes:\n",
    "   - Gr√°fico de barras: receita por ano\n",
    "   - Gr√°fico de pizza: vendas por categoria\n",
    "   - Tabela: m√©tricas de vendas por trimestre\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f7bdd",
   "metadata": {},
   "source": [
    "## 21. Exerc√≠cio 20: Consultas via Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a11021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 20: Simula√ß√£o de consultas Trino\n",
    "print(\"üîç Simulando consultas que seriam executadas via Trino...\")\n",
    "\n",
    "# Estas s√£o as consultas que voc√™ executaria no CLI do Trino ou Superset\n",
    "trino_queries = [\n",
    "    \"SELECT * FROM iceberg.lab.db.pessoas;\",\n",
    "    \"SELECT * FROM iceberg.lab.db.vendas ORDER BY ano, valor DESC;\",\n",
    "    \"SELECT ano, SUM(valor) as total FROM iceberg.lab.db.vendas GROUP BY ano;\",\n",
    "    \"SELECT categoria, AVG(preco) as preco_medio FROM iceberg.lab.db.tabela_df GROUP BY categoria;\"\n",
    "]\n",
    "\n",
    "print(\"üìã Consultas Trino equivalentes:\")\n",
    "for i, query in enumerate(trino_queries, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Executando consultas via Spark (equivalente ao Trino):\")\n",
    "\n",
    "# Query 1: Pessoas\n",
    "print(\"\\n1Ô∏è‚É£ Consulta: Tabela pessoas\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "\n",
    "# Query 2: Vendas ordenadas\n",
    "print(\"\\n2Ô∏è‚É£ Consulta: Vendas ordenadas por ano e valor\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas ORDER BY ano, valor DESC\").show()\n",
    "\n",
    "# Query 3: Total por ano\n",
    "print(\"\\n3Ô∏è‚É£ Consulta: Total de vendas por ano\")\n",
    "spark.sql(\"SELECT ano, SUM(valor) as total FROM iceberg.lab.db.vendas GROUP BY ano ORDER BY ano\").show()\n",
    "\n",
    "# Query 4: Pre√ßo m√©dio por categoria\n",
    "print(\"\\n4Ô∏è‚É£ Consulta: Pre√ßo m√©dio por categoria de produtos\")\n",
    "spark.sql(\"SELECT categoria, AVG(preco) as preco_medio FROM iceberg.lab.db.tabela_df GROUP BY categoria\").show()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° COMANDOS TRINO CLI:\n",
    "\n",
    "Para conectar ao Trino CLI (fora do notebook):\n",
    "$ trino --server localhost:8080 --catalog iceberg --schema lab.db\n",
    "\n",
    "Comandos √∫teis no Trino:\n",
    "- SHOW CATALOGS;\n",
    "- SHOW SCHEMAS FROM iceberg;\n",
    "- SHOW TABLES FROM iceberg.lab.db;\n",
    "- DESCRIBE iceberg.lab.db.vendas;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f141de4",
   "metadata": {},
   "source": [
    "## üéØ Resumo Final e Verifica√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fcc794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUMO FINAL - Verifica√ß√£o de todos os exerc√≠cios\n",
    "print(\"üéØ RESUMO FINAL DOS EXERC√çCIOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificar todas as tabelas criadas\n",
    "print(\"üìã Tabelas Iceberg criadas:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW TABLES IN iceberg.lab.db\").show()\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Erro ao listar tabelas\")\n",
    "\n",
    "print(\"\\nüìä Resumo dos dados:\")\n",
    "\n",
    "# 1. Tabela pessoas\n",
    "try:\n",
    "    pessoas_count = spark.sql(\"SELECT COUNT(*) FROM iceberg.lab.db.pessoas\").collect()[0][0]\n",
    "    print(f\"üë• Pessoas: {pessoas_count} registros\")\n",
    "except:\n",
    "    print(\"üë• Pessoas: Erro ao contar\")\n",
    "\n",
    "# 2. Tabela vendas\n",
    "try:\n",
    "    vendas_count = spark.sql(\"SELECT COUNT(*) FROM iceberg.lab.db.vendas\").collect()[0][0]\n",
    "    vendas_total = spark.sql(\"SELECT SUM(valor) FROM iceberg.lab.db.vendas\").collect()[0][0]\n",
    "    print(f\"üí∞ Vendas: {vendas_count} registros, Total: R$ {vendas_total:.2f}\")\n",
    "except:\n",
    "    print(\"üí∞ Vendas: Erro ao contar\")\n",
    "\n",
    "# 3. Tabela produtos\n",
    "try:\n",
    "    produtos_count = spark.sql(\"SELECT COUNT(*) FROM iceberg.lab.db.tabela_df\").collect()[0][0]\n",
    "    print(f\"üì¶ Produtos: {produtos_count} registros\")\n",
    "except:\n",
    "    print(\"üì¶ Produtos: Erro ao contar\")\n",
    "\n",
    "print(\"\\n‚úÖ EXERC√çCIOS COMPLETADOS:\")\n",
    "exercicios = [\n",
    "    \"‚úì 1. DataFrame simples criado\",\n",
    "    \"‚úì 2. DataFrame salvo no HDFS como CSV\", \n",
    "    \"‚úì 3. CSV lido do HDFS\",\n",
    "    \"‚úì 4. Namespace Iceberg criado\",\n",
    "    \"‚úì 5. Tabela Iceberg criada\",\n",
    "    \"‚úì 6. Dados inseridos na tabela Iceberg\",\n",
    "    \"‚úì 7. Tabela Iceberg consultada\",\n",
    "    \"‚úì 8. Registros contados\",\n",
    "    \"‚úì 9. Registro atualizado\",\n",
    "    \"‚úì 10. Registro deletado\",\n",
    "    \"‚úì 11. Tabela particionada criada\",\n",
    "    \"‚úì 12. Dados particionados inseridos\",\n",
    "    \"‚úì 13. Parti√ß√£o espec√≠fica consultada\",\n",
    "    \"‚úì 14. Metadados da tabela visualizados\",\n",
    "    \"‚úì 15. Tabela criada a partir de DataFrame\",\n",
    "    \"‚úì 16. Convers√£o para Iceberg demonstrada\",\n",
    "    \"‚úì 17. Time Travel implementado\",\n",
    "    \"‚úì 18. Dados exportados para CSV\",\n",
    "    \"‚úì 19. Prepara√ß√£o para Superset realizada\",\n",
    "    \"‚úì 20. Consultas Trino simuladas\"\n",
    "]\n",
    "\n",
    "for exercicio in exercicios:\n",
    "    print(exercicio)\n",
    "\n",
    "print(f\"\\nüéâ PARAB√âNS! Todos os {len(exercicios)} exerc√≠cios foram completados!\")\n",
    "print(\"\\nüí° PR√ìXIMOS PASSOS:\")\n",
    "print(\"- Conecte o Superset ao Trino para criar dashboards\")\n",
    "print(\"- Explore funcionalidades avan√ßadas do Iceberg\")\n",
    "print(\"- Pratique com datasets maiores\")\n",
    "print(\"- Implemente pipelines de dados completos\")\n",
    "\n",
    "print(\"\\nüöÄ Bom estudo com Big Data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 10: Deletar um registro\n",
    "print(\"üóëÔ∏è Deletando registro da tabela pessoas...\")\n",
    "\n",
    "# Mostrar dados antes da dele√ß√£o\n",
    "print(\"üìä Dados ANTES da dele√ß√£o:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "\n",
    "# Deletar Charlie\n",
    "delete_sql = \"\"\"\n",
    "DELETE FROM iceberg.lab.db.pessoas \n",
    "WHERE nome = 'Charlie'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(delete_sql)\n",
    "print(\"‚úÖ Registro deletado com sucesso!\")\n",
    "\n",
    "# Mostrar dados ap√≥s a dele√ß√£o\n",
    "print(\"üìä Dados AP√ìS a dele√ß√£o:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "\n",
    "# Contar registros restantes\n",
    "total_after_delete = spark.sql(\"SELECT COUNT(*) as total FROM iceberg.lab.db.pessoas\").collect()[0]['total']\n",
    "print(f\"üìà Total de registros ap√≥s dele√ß√£o: {total_after_delete}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50589cc",
   "metadata": {},
   "source": [
    "## 12. Exerc√≠cio 11: Criar tabela particionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ed7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 11: Criar tabela particionada\n",
    "print(\"üóÇÔ∏è Criando tabela particionada 'vendas'...\")\n",
    "\n",
    "create_partitioned_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg.lab.db.vendas (\n",
    "    id INT,\n",
    "    valor DOUBLE,\n",
    "    ano INT\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (ano)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_partitioned_table_sql)\n",
    "print(\"‚úÖ Tabela particionada 'vendas' criada com sucesso!\")\n",
    "\n",
    "# Verificar estrutura da tabela\n",
    "print(\"\\nüìä Estrutura da tabela vendas:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lab.db.vendas\").show()\n",
    "\n",
    "# Mostrar informa√ß√µes de particionamento\n",
    "print(\"\\nüóÇÔ∏è Informa√ß√µes de particionamento:\")\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE DETAIL iceberg.lab.db.vendas\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel mostrar detalhes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe8007",
   "metadata": {},
   "source": [
    "## 13. Exerc√≠cio 12: Inserir dados particionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 12: Inserir dados particionados\n",
    "print(\"üìù Inserindo dados particionados na tabela vendas...\")\n",
    "\n",
    "# Inserir dados com diferentes anos\n",
    "insert_vendas_sql = \"\"\"\n",
    "INSERT INTO iceberg.lab.db.vendas VALUES \n",
    "    (1, 1500.50, 2022),\n",
    "    (2, 2300.75, 2022),\n",
    "    (3, 1800.00, 2023),\n",
    "    (4, 2100.25, 2023),\n",
    "    (5, 2500.00, 2023),\n",
    "    (6, 1900.00, 2024),\n",
    "    (7, 2200.50, 2024)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(insert_vendas_sql)\n",
    "print(\"‚úÖ Dados inseridos com sucesso!\")\n",
    "\n",
    "# Mostrar todos os dados\n",
    "print(\"\\nüìä Dados na tabela vendas:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas ORDER BY ano, id\").show()\n",
    "\n",
    "# Mostrar resumo por ano\n",
    "print(\"\\nüìà Resumo por ano:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT ano, COUNT(*) as total_vendas, SUM(valor) as total_valor \n",
    "    FROM iceberg.lab.db.vendas \n",
    "    GROUP BY ano \n",
    "    ORDER BY ano\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b016f45",
   "metadata": {},
   "source": [
    "## 14. Exerc√≠cio 13: Consultar apenas um particionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 13: Consultar apenas um particionamento\n",
    "print(\"üîç Consultando apenas vendas de 2023...\")\n",
    "\n",
    "# Query para apenas 2023\n",
    "vendas_2023 = spark.sql(\"\"\"\n",
    "    SELECT * FROM iceberg.lab.db.vendas \n",
    "    WHERE ano = 2023\n",
    "    ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "vendas_2023.show()\n",
    "\n",
    "print(f\"üìä Total de vendas em 2023: {vendas_2023.count()}\")\n",
    "\n",
    "# Mostrar estat√≠sticas de 2023\n",
    "print(\"\\nüìà Estat√≠sticas de vendas 2023:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_vendas,\n",
    "        SUM(valor) as valor_total,\n",
    "        AVG(valor) as valor_medio,\n",
    "        MIN(valor) as menor_venda,\n",
    "        MAX(valor) as maior_venda\n",
    "    FROM iceberg.lab.db.vendas \n",
    "    WHERE ano = 2023\n",
    "\"\"\").show()\n",
    "\n",
    "# Verificar se o Spark est√° fazendo partition pruning\n",
    "print(\"\\nüöÄ Plano de execu√ß√£o (deve mostrar partition pruning):\")\n",
    "vendas_2023.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94b479",
   "metadata": {},
   "source": [
    "## 15. Exerc√≠cio 14: Ver metadados da tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 14: Ver metadados da tabela\n",
    "print(\"üìã Verificando metadados da tabela vendas...\")\n",
    "\n",
    "# DESCRIBE HISTORY - mostra hist√≥rico de commits/snapshots\n",
    "print(\"\\nüïí Hist√≥rico da tabela (DESCRIBE HISTORY):\")\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE HISTORY iceberg.lab.db.vendas\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao obter hist√≥rico: {e}\")\n",
    "\n",
    "# DESCRIBE DETAIL - mostra detalhes da tabela\n",
    "print(\"\\nüîç Detalhes da tabela (DESCRIBE DETAIL):\")\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE DETAIL iceberg.lab.db.vendas\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao obter detalhes: {e}\")\n",
    "\n",
    "# Informa√ß√µes adicionais do Iceberg\n",
    "print(\"\\nüìä Estrutura da tabela:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lab.db.vendas\").show()\n",
    "\n",
    "# Mostrar arquivos da tabela (se dispon√≠vel)\n",
    "print(\"\\nüìÅ Files da tabela:\")\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM iceberg.lab.db.vendas.files\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel mostrar files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462b74a",
   "metadata": {},
   "source": [
    "## 16. Exerc√≠cio 15: Criar tabela Iceberg a partir de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 15: Criar tabela Iceberg a partir de DataFrame\n",
    "print(\"üìä Criando DataFrame artificial e salvando como tabela Iceberg...\")\n",
    "\n",
    "# Criar um DataFrame com dados de produtos\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "produtos_data = [\n",
    "    (1, \"Notebook\", 2500.00, \"Eletr√¥nicos\", datetime(2024, 1, 15)),\n",
    "    (2, \"Mouse\", 45.50, \"Eletr√¥nicos\", datetime(2024, 1, 16)),\n",
    "    (3, \"Teclado\", 120.00, \"Eletr√¥nicos\", datetime(2024, 1, 17)),\n",
    "    (4, \"Monitor\", 800.00, \"Eletr√¥nicos\", datetime(2024, 1, 18)),\n",
    "    (5, \"Cadeira\", 350.00, \"M√≥veis\", datetime(2024, 1, 19)),\n",
    "    (6, \"Mesa\", 450.00, \"M√≥veis\", datetime(2024, 1, 20))\n",
    "]\n",
    "\n",
    "produtos_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"preco\", DoubleType(), True),\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"data_cadastro\", DateType(), True)\n",
    "])\n",
    "\n",
    "df_produtos = spark.createDataFrame(produtos_data, produtos_schema)\n",
    "\n",
    "print(\"üìã DataFrame criado:\")\n",
    "df_produtos.show()\n",
    "\n",
    "# Salvar como tabela Iceberg usando writeTo\n",
    "print(\"\\nüíæ Salvando como tabela Iceberg...\")\n",
    "df_produtos.writeTo(\"iceberg.lab.db.produtos\").createOrReplace()\n",
    "\n",
    "print(\"‚úÖ Tabela 'produtos' criada com sucesso!\")\n",
    "\n",
    "# Verificar a tabela criada\n",
    "print(\"\\nüìä Dados da nova tabela:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.produtos\").show()\n",
    "\n",
    "print(\"\\nüìã Schema da tabela:\")\n",
    "spark.sql(\"DESCRIBE iceberg.lab.db.produtos\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8a7b8",
   "metadata": {},
   "source": [
    "## 17. Exerc√≠cio 16: Converter tabela para Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b09747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 16: Converter tabela para Iceberg\n",
    "print(\"üîÑ Criando tabela Parquet e convertendo para Iceberg...\")\n",
    "\n",
    "# Primeiro, criar uma tabela Parquet simples\n",
    "clientes_data = [\n",
    "    (1, \"Jo√£o Silva\", \"joao@email.com\", \"SP\"),\n",
    "    (2, \"Maria Santos\", \"maria@email.com\", \"RJ\"),\n",
    "    (3, \"Pedro Costa\", \"pedro@email.com\", \"MG\"),\n",
    "    (4, \"Ana Oliveira\", \"ana@email.com\", \"SP\")\n",
    "]\n",
    "\n",
    "clientes_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_clientes = spark.createDataFrame(clientes_data, clientes_schema)\n",
    "\n",
    "# Salvar como tabela Parquet no cat√°logo Hive\n",
    "print(\"üìÅ Criando tabela Parquet...\")\n",
    "df_clientes.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"hdfs://namenode:9000/warehouse/clientes_parquet\") \\\n",
    "    .saveAsTable(\"default.clientes_parquet\")\n",
    "\n",
    "print(\"‚úÖ Tabela Parquet criada!\")\n",
    "\n",
    "# Verificar a tabela Parquet\n",
    "print(\"\\nüìä Dados da tabela Parquet:\")\n",
    "spark.sql(\"SELECT * FROM default.clientes_parquet\").show()\n",
    "\n",
    "# Agora converter para Iceberg\n",
    "print(\"\\nüîÑ Convertendo para Iceberg...\")\n",
    "try:\n",
    "    # Criar tabela Iceberg baseada na Parquet\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE iceberg.lab.db.clientes\n",
    "        USING ICEBERG\n",
    "        AS SELECT * FROM default.clientes_parquet\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ Tabela convertida para Iceberg com sucesso!\")\n",
    "    \n",
    "    # Verificar a tabela Iceberg\n",
    "    print(\"\\nüìä Dados da tabela Iceberg:\")\n",
    "    spark.sql(\"SELECT * FROM iceberg.lab.db.clientes\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro na convers√£o: {e}\")\n",
    "    print(\"üí° Tentando m√©todo alternativo...\")\n",
    "    \n",
    "    # M√©todo alternativo: ler Parquet e escrever como Iceberg\n",
    "    df_from_parquet = spark.sql(\"SELECT * FROM default.clientes_parquet\")\n",
    "    df_from_parquet.writeTo(\"iceberg.lab.db.clientes\").createOrReplace()\n",
    "    print(\"‚úÖ Convers√£o alternativa realizada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4adbdd",
   "metadata": {},
   "source": [
    "## 18. Exerc√≠cio 17: Leitura incremental (Time Travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 17: Leitura incremental (Time Travel)\n",
    "print(\"‚è∞ Demonstrando Time Travel com Iceberg...\")\n",
    "\n",
    "# Primeiro, vamos ver o hist√≥rico da tabela vendas\n",
    "print(\"üìã Hist√≥rico atual da tabela vendas:\")\n",
    "try:\n",
    "    history_df = spark.sql(\"DESCRIBE HISTORY iceberg.lab.db.vendas\")\n",
    "    history_df.show()\n",
    "    \n",
    "    # Pegar o n√∫mero de snapshots dispon√≠veis\n",
    "    snapshots = history_df.collect()\n",
    "    print(f\"üìä Total de snapshots dispon√≠veis: {len(snapshots)}\")\n",
    "    \n",
    "    if len(snapshots) >= 2:\n",
    "        # Mostrar dados atuais\n",
    "        print(\"\\nüìä Dados atuais (vers√£o mais recente):\")\n",
    "        spark.sql(\"SELECT * FROM iceberg.lab.db.vendas ORDER BY id\").show()\n",
    "        \n",
    "        # Time travel para vers√£o anterior\n",
    "        print(f\"\\n‚è™ Dados da vers√£o anterior (snapshot {snapshots[1]['snapshot_id']}):\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT * FROM iceberg.lab.db.vendas \n",
    "            VERSION AS OF {snapshots[1]['snapshot_id']}\n",
    "            ORDER BY id\n",
    "        \"\"\").show()\n",
    "        \n",
    "        # Time travel por timestamp\n",
    "        if len(snapshots) >= 1:\n",
    "            older_timestamp = snapshots[-1]['made_current_at']\n",
    "            print(f\"\\nüïí Dados em timestamp espec√≠fico ({older_timestamp}):\")\n",
    "            spark.sql(f\"\"\"\n",
    "                SELECT * FROM iceberg.lab.db.vendas \n",
    "                TIMESTAMP AS OF '{older_timestamp}'\n",
    "                ORDER BY id\n",
    "            \"\"\").show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Poucos snapshots dispon√≠veis para demonstrar time travel\")\n",
    "        \n",
    "        # Vamos criar mais mudan√ßas para ter mais snapshots\n",
    "        print(\"üìù Criando mais altera√ß√µes para demonstrar time travel...\")\n",
    "        \n",
    "        # Adicionar mais dados\n",
    "        spark.sql(\"\"\"\n",
    "            INSERT INTO iceberg.lab.db.vendas VALUES \n",
    "                (8, 3000.00, 2024),\n",
    "                (9, 1750.00, 2024)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Fazer uma atualiza√ß√£o\n",
    "        spark.sql(\"\"\"\n",
    "            UPDATE iceberg.lab.db.vendas \n",
    "            SET valor = valor * 1.1 \n",
    "            WHERE ano = 2024\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"‚úÖ Novas altera√ß√µes feitas!\")\n",
    "        \n",
    "        # Mostrar hist√≥rico atualizado\n",
    "        print(\"\\nüìã Hist√≥rico atualizado:\")\n",
    "        spark.sql(\"DESCRIBE HISTORY iceberg.lab.db.vendas\").show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao demonstrar time travel: {e}\")\n",
    "    print(\"üí° Time travel requer snapshots m√∫ltiplos da tabela\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe0f21",
   "metadata": {},
   "source": [
    "## 19. Exerc√≠cio 18: Exportar tabela Iceberg para CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f39f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 18: Exportar tabela Iceberg para CSV\n",
    "print(\"üì§ Exportando tabela Iceberg para CSV no HDFS...\")\n",
    "\n",
    "# Ler dados da tabela Iceberg vendas\n",
    "df_vendas_export = spark.sql(\"SELECT * FROM iceberg.lab.db.vendas\")\n",
    "\n",
    "print(\"üìä Dados a serem exportados:\")\n",
    "df_vendas_export.show()\n",
    "\n",
    "# Caminho de destino no HDFS\n",
    "export_path = \"hdfs://namenode:9000/export/vendas.csv\"\n",
    "\n",
    "# Exportar para CSV\n",
    "df_vendas_export.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(export_path)\n",
    "\n",
    "print(f\"‚úÖ Dados exportados com sucesso para: {export_path}\")\n",
    "\n",
    "# Verificar se o arquivo foi criado\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-ls', '/export/'], \n",
    "                          capture_output=True, text=True)\n",
    "    print(\"\\nüìÅ Arquivos no diret√≥rio /export/:\")\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel listar arquivos: {e}\")\n",
    "\n",
    "# Verificar conte√∫do do arquivo exportado\n",
    "print(\"\\nüîç Verificando dados exportados:\")\n",
    "df_verificacao = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(export_path)\n",
    "\n",
    "df_verificacao.show()\n",
    "print(f\"üìà Total de registros exportados: {df_verificacao.count()}\")\n",
    "\n",
    "# Estat√≠sticas da exporta√ß√£o\n",
    "print(\"\\nüìä Estat√≠sticas da exporta√ß√£o:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Original' as fonte,\n",
    "        COUNT(*) as total_registros,\n",
    "        SUM(valor) as soma_valores,\n",
    "        AVG(valor) as valor_medio\n",
    "    FROM iceberg.lab.db.vendas\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"‚úÖ Exerc√≠cio 18 conclu√≠do: Tabela Iceberg exportada para CSV com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cbf7e",
   "metadata": {},
   "source": [
    "## 20. Exerc√≠cio 19: Configura√ß√£o para Dashboard no Superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95054907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 19: Prepara√ß√£o para Dashboard no Superset\n",
    "print(\"üìä Preparando dados para Dashboard no Superset...\")\n",
    "\n",
    "# Criar uma view agregada para facilitar visualiza√ß√µes\n",
    "print(\"üìà Criando view agregada para dashboards...\")\n",
    "\n",
    "create_view_sql = \"\"\"\n",
    "CREATE OR REPLACE VIEW iceberg.lab.db.vendas_dashboard AS\n",
    "SELECT \n",
    "    ano,\n",
    "    COUNT(*) as total_vendas,\n",
    "    SUM(valor) as receita_total,\n",
    "    AVG(valor) as ticket_medio,\n",
    "    MIN(valor) as menor_venda,\n",
    "    MAX(valor) as maior_venda\n",
    "FROM iceberg.lab.db.vendas\n",
    "GROUP BY ano\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_view_sql)\n",
    "print(\"‚úÖ View 'vendas_dashboard' criada!\")\n",
    "\n",
    "# Verificar dados da view\n",
    "print(\"\\nüìä Dados da view para dashboard:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas_dashboard ORDER BY ano\").show()\n",
    "\n",
    "# Criar dados de exemplo adicionais para visualiza√ß√µes mais ricas\n",
    "print(\"\\nüìù Criando tabela de vendas detalhadas para dashboards...\")\n",
    "\n",
    "vendas_detalhadas_sql = \"\"\"\n",
    "CREATE OR REPLACE TABLE iceberg.lab.db.vendas_detalhadas\n",
    "USING ICEBERG AS\n",
    "SELECT \n",
    "    id,\n",
    "    valor,\n",
    "    ano,\n",
    "    CASE \n",
    "        WHEN valor < 1000 THEN 'Baixo'\n",
    "        WHEN valor < 2000 THEN 'M√©dio' \n",
    "        ELSE 'Alto'\n",
    "    END as categoria_valor,\n",
    "    CASE \n",
    "        WHEN id % 4 = 0 THEN 'Q1'\n",
    "        WHEN id % 4 = 1 THEN 'Q2'\n",
    "        WHEN id % 4 = 2 THEN 'Q3'\n",
    "        ELSE 'Q4'\n",
    "    END as trimestre,\n",
    "    valor * 0.1 as comissao\n",
    "FROM iceberg.lab.db.vendas\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(vendas_detalhadas_sql)\n",
    "print(\"‚úÖ Tabela 'vendas_detalhadas' criada!\")\n",
    "\n",
    "print(\"\\nüìä Preview dos dados detalhados:\")\n",
    "spark.sql(\"SELECT * FROM iceberg.lab.db.vendas_detalhadas ORDER BY ano, id\").show()\n",
    "\n",
    "print(\"\"\"\n",
    "üìã INSTRU√á√ïES PARA SUPERSET:\n",
    "\n",
    "1. Conectar ao Trino:\n",
    "   - Host: trino (ou IP do container Trino)\n",
    "   - Porta: 8080\n",
    "   - Usu√°rio: admin\n",
    "   - Cat√°logo: iceberg\n",
    "   - Schema: lab.db\n",
    "\n",
    "2. URL de conex√£o no Superset:\n",
    "   trino://admin@trino:8080/iceberg/lab.db\n",
    "\n",
    "3. Tabelas dispon√≠veis para visualiza√ß√£o:\n",
    "   - iceberg.lab.db.vendas (dados brutos)\n",
    "   - iceberg.lab.db.vendas_dashboard (dados agregados)\n",
    "   - iceberg.lab.db.vendas_detalhadas (dados com categorias)\n",
    "   - iceberg.lab.db.pessoas (dados de pessoas)\n",
    "\n",
    "4. Exemplos de visualiza√ß√µes:\n",
    "   - Gr√°fico de barras: receita por ano\n",
    "   - Gr√°fico de pizza: vendas por categoria\n",
    "   - Tabela: m√©tricas de vendas por trimestre\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5927d5",
   "metadata": {},
   "source": [
    "## 21. Exerc√≠cio 20: Consultas via Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exerc√≠cio 20: Simula√ß√£o de consultas Trino\n",
    "print(\"üîç Simulando consultas que seriam executadas via Trino...\")\n",
    "\n",
    "# Estas s√£o as consultas que voc√™ executaria no CLI do Trino ou Superset\n",
    "trino_queries = [\n",
    "    \"SELECT * FROM iceberg.lab.db.pessoas;\",\n",
    "    \"SELECT * FROM iceberg.lab.db.vendas ORDER BY ano, valor DESC;\",\n",
    "    \"SELECT ano, SUM(valor) as total FROM iceberg.lab.db.vendas GROUP BY ano;\",\n",
    "    \"SELECT categoria, AVG(preco) as preco_medio FROM iceberg.lab.db.produtos GROUP BY categoria;\"\n",
    "]\n",
    "\n",
    "print(\"üìã Consultas Trino equivalentes:\")\n",
    "for i, query in enumerate(trino_queries, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Executando consultas via Spark (equivalente ao Trino):\")\n",
    "\n",
    "# Query 1: Pessoas\n",
    "print(\"\\n1Ô∏è‚É£ Consulta: Tabela pessoas\")\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM iceberg.lab.db.pessoas\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro: {e}\")\n",
    "\n",
    "# Query 2: Vendas ordenadas\n",
    "print(\"\\n2Ô∏è‚É£ Consulta: Vendas ordenadas por ano e valor\")\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM iceberg.lab.db.vendas ORDER BY ano, valor DESC\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro: {e}\")\n",
    "\n",
    "# Query 3: Total por ano\n",
    "print(\"\\n3Ô∏è‚É£ Consulta: Total de vendas por ano\")\n",
    "try:\n",
    "    spark.sql(\"SELECT ano, SUM(valor) as total FROM iceberg.lab.db.vendas GROUP BY ano ORDER BY ano\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro: {e}\")\n",
    "\n",
    "# Query 4: Pre√ßo m√©dio por categoria\n",
    "print(\"\\n4Ô∏è‚É£ Consulta: Pre√ßo m√©dio por categoria de produtos\")\n",
    "try:\n",
    "    spark.sql(\"SELECT categoria, AVG(preco) as preco_medio FROM iceberg.lab.db.produtos GROUP BY categoria\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro: {e}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° COMANDOS TRINO CLI:\n",
    "\n",
    "Para conectar ao Trino CLI (fora do notebook):\n",
    "$ trino --server localhost:8080 --catalog iceberg --schema lab.db\n",
    "\n",
    "Comandos √∫teis no Trino:\n",
    "- SHOW CATALOGS;\n",
    "- SHOW SCHEMAS FROM iceberg;\n",
    "- SHOW TABLES FROM iceberg.lab.db;\n",
    "- DESCRIBE iceberg.lab.db.vendas;\n",
    "\n",
    "‚úÖ Exerc√≠cio 20 conclu√≠do: Consultas Trino simuladas com sucesso!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb42a6",
   "metadata": {},
   "source": [
    "## üéØ Resumo Final Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a944181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ RESUMO FINAL - Verifica√ß√£o de todos os 20 exerc√≠cios\n",
    "print(\"üéØ RESUMO FINAL DOS EXERC√çCIOS DE BIG DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar todas as tabelas criadas\n",
    "print(\"üìã Verificando tabelas Iceberg criadas:\")\n",
    "try:\n",
    "    spark.sql(\"SHOW TABLES IN iceberg.lab.db\").show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao listar tabelas: {e}\")\n",
    "\n",
    "print(\"\\nüìä Resumo dos dados:\")\n",
    "\n",
    "# Verifica√ß√£o das tabelas principais\n",
    "tabelas_para_verificar = [\n",
    "    (\"pessoas\", \"iceberg.lab.db.pessoas\"),\n",
    "    (\"vendas\", \"iceberg.lab.db.vendas\"),\n",
    "    (\"produtos\", \"iceberg.lab.db.produtos\"),\n",
    "    (\"vendas_dashboard\", \"iceberg.lab.db.vendas_dashboard\"),\n",
    "    (\"vendas_detalhadas\", \"iceberg.lab.db.vendas_detalhadas\")\n",
    "]\n",
    "\n",
    "for nome, tabela in tabelas_para_verificar:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) FROM {tabela}\").collect()[0][0]\n",
    "        print(f\"‚úÖ {nome}: {count} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {nome}: Erro ao verificar - {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ LISTA COMPLETA DOS 20 EXERC√çCIOS:\")\n",
    "exercicios = [\n",
    "    \"‚úì  1. DataFrame simples criado\",\n",
    "    \"‚úì  2. DataFrame salvo no HDFS como CSV\", \n",
    "    \"‚úì  3. CSV lido do HDFS\",\n",
    "    \"‚úì  4. Namespace Iceberg criado\",\n",
    "    \"‚úì  5. Tabela Iceberg criada\",\n",
    "    \"‚úì  6. Dados inseridos na tabela Iceberg\",\n",
    "    \"‚úì  7. Tabela Iceberg consultada\",\n",
    "    \"‚úì  8. Registros contados\",\n",
    "    \"‚úì  9. Registro atualizado\",\n",
    "    \"‚úì 10. Registro deletado\",\n",
    "    \"‚úì 11. Tabela particionada criada\",\n",
    "    \"‚úì 12. Dados particionados inseridos\",\n",
    "    \"‚úì 13. Parti√ß√£o espec√≠fica consultada\",\n",
    "    \"‚úì 14. Metadados da tabela visualizados\",\n",
    "    \"‚úì 15. Tabela criada a partir de DataFrame\",\n",
    "    \"‚úì 16. Convers√£o para Iceberg demonstrada\",\n",
    "    \"‚úì 17. Time Travel implementado\",\n",
    "    \"‚úì 18. Dados exportados para CSV\",\n",
    "    \"‚úì 19. Prepara√ß√£o para Superset realizada\",\n",
    "    \"‚úì 20. Consultas Trino simuladas\"\n",
    "]\n",
    "\n",
    "for exercicio in exercicios:\n",
    "    print(exercicio)\n",
    "\n",
    "print(f\"\\nüéâ PARAB√âNS! Todos os {len(exercicios)} exerc√≠cios foram completados!\")\n",
    "\n",
    "print(\"\\nüí° TECNOLOGIAS UTILIZADAS:\")\n",
    "tecnologias = [\n",
    "    \"üêç Python + PySpark\",\n",
    "    \"üßä Apache Iceberg (formato de tabela)\",\n",
    "    \"üóÑÔ∏è Hive Metastore (metadados)\",\n",
    "    \"üêò HDFS (armazenamento distribu√≠do)\",\n",
    "    \"üîç Trino (engine de consulta)\",\n",
    "    \"üìä Apache Superset (dashboards)\",\n",
    "    \"‚ö° Spark SQL (processamento)\"\n",
    "]\n",
    "\n",
    "for tech in tecnologias:\n",
    "    print(f\"   {tech}\")\n",
    "\n",
    "print(\"\\nüöÄ PR√ìXIMOS PASSOS SUGERIDOS:\")\n",
    "next_steps = [\n",
    "    \"1. Execute este notebook no ambiente lab\",\n",
    "    \"2. Conecte o Superset ao Trino para criar dashboards\",\n",
    "    \"3. Explore funcionalidades avan√ßadas do Iceberg\",\n",
    "    \"4. Pratique com datasets maiores e mais complexos\",\n",
    "    \"5. Implemente pipelines de dados end-to-end\",\n",
    "    \"6. Estude otimiza√ß√µes de performance com particionamento\",\n",
    "    \"7. Experimente com schema evolution no Iceberg\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéì NOTEBOOK DE EXERC√çCIOS COMPLETO!\")\n",
    "print(\"üìö Bom estudo com Big Data, Spark e Iceberg!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
